{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHEv01Di1Wlx"
      },
      "source": [
        "# Big Data e Processamento Distribuído\n",
        "## Projeto de Disciplina"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST6-hWHB1Ywp"
      },
      "source": [
        "Alunos:\n",
        "- André N. Darcie\n",
        "- Cristiane Lemos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF60_wPn13Sx"
      },
      "source": [
        "## Atividade\n",
        "**Valor**: 50 pontos    \n",
        "**Entrega**:  16/01 às 23:59     \n",
        "**Formato**: Jupyter Notebook     \n",
        "Individual ou em duplas.     \n",
        "**Objetivo**: realizar um ciclo de ciência de dados completo no Spark.      \n",
        " \n",
        "Nesse  projeto,  vocês  deverão  realizar  um  ciclo  completo  de  ciência  de  dados  utilizando  o \n",
        "PySpark.  Isso  significa  que  vocês  deverão  explorar  e  preparar  dados,  treinar  um  modelo  de \n",
        "aprendizado de máquina e fazer análise dos resultados obtidos.  \n",
        "Para escolher um dataset, vocês poderão visitar o Kaggle e selecionar algum que estejam com \n",
        "vontade de explorar. Vocês deverão optar por datasets apropriados para as seguintes tarefas: \n",
        "• Classificação \n",
        "• Agrupamento \n",
        "• Recomendação \n",
        "\n",
        "Todo  o  projeto  deve  ser  construído  em  um  único  Notebook.  Nele  deverão  conter,  além  do \n",
        "código,  análises,  explicações  e  motivações  para  a  escolha  do  dataset  e  do  algoritmo  de \n",
        "aprendizado de máquina. IMPORTANTE: Todos os imports utilizados deverão ser colocados no \n",
        "início do Notebook.  \n",
        " \n",
        "### Parte I: Exploração de Dados.\n",
        "\n",
        "Vocês  deverão  utilizar  as  funcionalidades  de  RDD  e/ou  Dataframes  para  analisar  e  limpar  os \n",
        "dados. Como essa tarefa é dependente de cada conjunto de dados, não há um modelo rígido a \n",
        "seguir. Porém, vocês deverão realizar no mínimo 2 análises (estatísticas, análise com gráficos, \n",
        "etc.) e 3 transformações (filtragem, remoção de características, remoção/troca de valores nulos, \n",
        "normalizações, etc). As transformações devem ser pautadas no que for descoberto ao analisar \n",
        "os dados. Por exemplo: normalização dos valores por discrepância de magnitude entre \n",
        "características. \n",
        " \n",
        "### Parte II: Criação de um Modelo e Análise de Resultados. \n",
        "\n",
        "Nessa etapa, vocês deverão rodar um algoritmo da biblioteca MLlib do Spark para aprender um \n",
        "modelo  de  aprendizado  de  máquina  com  os  dados  que  vocês  acabaram  de  organizar.  Vocês \n",
        "deverão motivar a escolha do algoritmo, que deve ser um dos disponíveis dentro da MLlib do \n",
        "Spark. Além disso, vocês deverão dividir os dados utilizando alguma metodologia de validação \n",
        "(cross-validation,  60-40,  80-20,  etc),  e  validar  a  performance  do  seu  modelo,  analisando  os \n",
        "resultados. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4vtbQzV3OHI"
      },
      "source": [
        "## Configuração do ambiente Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APsHuzTE3Cju"
      },
      "source": [
        "Esse notebook é baseado no notebook [como instalar pyspark no google colab](https://colab.research.google.com/github/carlosfab/sigmoidal_ai/blob/master/Big_Data_Como_instalar_o_PySpark_no_Google_Colab.ipynb).       \n",
        "O ambiente de execução é configurado para executar no Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iyig3sF93LM5",
        "outputId": "62cea334-b6f1-4182-f58f-ec18368d3a63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "# instalar as dependências\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# configurar as variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "# tornar o pyspark \"importável\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')\n",
        "\n",
        "import pyspark\n",
        "\n",
        "sc = pyspark.SparkContext(appName='Projeto_BDPD')\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "FlV-ngc6X61K"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importação das bibliotecas utilizadas"
      ],
      "metadata": {
        "id": "tACpTIHaEBDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANTE: Todos os imports utilizados deverão ser colocados no início do Notebook.\n",
        "\n",
        "from pyspark.sql.functions import mean\n",
        "from pyspark.ml.feature import (VectorAssembler,OneHotEncoder,StringIndexer)\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.classification import NaiveBayes"
      ],
      "metadata": {
        "id": "BQSfWWvODMUr"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sobre o Dataset"
      ],
      "metadata": {
        "id": "79hO7muqESVy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkUfU-Vf3ZUw"
      },
      "source": [
        "### Dataset escolhido\n",
        "\n",
        "Healthcare Dataset Stroke Data -\n",
        "[link para o kaggle](https://www.kaggle.com/fedesoriano/stroke-prediction-dataset)\n",
        "\n",
        "### Contexto\n",
        "\n",
        "De acordo com a Organização Mundial da Saúde (OMS) o AVC é a 2ª causa de morte no mundo, responsável por aproximadamente 11% do total de mortes.\n",
        "Esse conjunto de dados é usado para prever se um paciente provavelmente sofrerá AVC com base nos parâmetros de entrada, como sexo, idade, várias doenças e tabagismo. Cada linha nos dados fornece informações relevantes sobre o paciente.\n",
        "\n",
        "### Informações sobre os atributos\n",
        "\n",
        "1) id: identificador único  \n",
        "2) sexo: \"Masculino\", \"Feminino\" ou \"Outro\"  \n",
        "3) idade: idade do paciente  \n",
        "4) hipertensão: 0 se o paciente não tiver hipertensão, 1 se o paciente tiver   hipertensão  \n",
        "5 ) heart_disease: 0 se o paciente não tiver nenhuma doença cardíaca, 1 se o paciente tiver uma doença cardíaca  \n",
        "6) ever_married: \"Não\" ou \"Sim\"  \n",
        "7) work_type: \"children\", \"Govt_jov\", \"Nunca_worked\", \"Particular\" ou \"Independente\"  \n",
        "8) Residence_type: \"Rural\" ou \"Urban\"  \n",
        "9) avg_glucose_level: nível médio de glicose no sangue  \n",
        "10) IMC: índice de massa corporal  \n",
        "11) tabagismo_status: \"ex-fumou\", \"nunca fumou\", \"fuma\"ou \"Desconhecido\"*  \n",
        "12) acidente vascular cerebral: 1 se o paciente teve um acidente vascular cerebral ou 0 se não  \n",
        "*Observação: \"Desconhecido\" em smoking_status significa que a informação não está disponível para este paciente  \n",
        "\n",
        "### Créditos\n",
        "\n",
        "Esse dataset foi publicado por [fedesoriano](https://www.kaggle.com/fedesoriano) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Motivação para escolha"
      ],
      "metadata": {
        "id": "lja_VsopF-FE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "É um dataset facil de entender o contexto e o significado de cada coluna e o seu objetivo, tem um tema relevante que é o da saúde, tornando o interessante na hora de investigar e explorar, e finalmente é um dataset popular no Kaggle oque significa que facilita na hora de encontrar material de referencia."
      ],
      "metadata": {
        "id": "by2CgvkCGJXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importação do Dataset"
      ],
      "metadata": {
        "id": "83VFSyoFHu-K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "CkWTuzEqbs1W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a966748-5c8f-4e09-a5e7-f413f56396bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'my-data-science-notebooks' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# coloquei em um repositorio do github para facilitar o download\n",
        "!git clone https://github.com/andredarcie/my-data-science-notebooks.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "sWRLP-nob3q3"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv('/content/my-data-science-notebooks/big-data-processamento-distribuido/datasets/healthcare-dataset-stroke-data.csv', inferSchema=True, header=True, nullValue='NA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Z4ylVPyGdVue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd7bbc7c-16dd-492a-c3fb-1cee6ade3d17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
            "|   id|gender| age|hypertension|heart_disease|ever_married|    work_type|Residence_type|avg_glucose_level| bmi| smoking_status|stroke|\n",
            "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
            "| 9046|  Male|67.0|           0|            1|         Yes|      Private|         Urban|           228.69|36.6|formerly smoked|     1|\n",
            "|51676|Female|61.0|           0|            0|         Yes|Self-employed|         Rural|           202.21| N/A|   never smoked|     1|\n",
            "|31112|  Male|80.0|           0|            1|         Yes|      Private|         Rural|           105.92|32.5|   never smoked|     1|\n",
            "|60182|Female|49.0|           0|            0|         Yes|      Private|         Urban|           171.23|34.4|         smokes|     1|\n",
            "| 1665|Female|79.0|           1|            0|         Yes|Self-employed|         Rural|           174.12|  24|   never smoked|     1|\n",
            "|56669|  Male|81.0|           0|            0|         Yes|      Private|         Urban|           186.21|  29|formerly smoked|     1|\n",
            "|53882|  Male|74.0|           1|            1|         Yes|      Private|         Rural|            70.09|27.4|   never smoked|     1|\n",
            "|10434|Female|69.0|           0|            0|          No|      Private|         Urban|            94.39|22.8|   never smoked|     1|\n",
            "|27419|Female|59.0|           0|            0|         Yes|      Private|         Rural|            76.15| N/A|        Unknown|     1|\n",
            "|60491|Female|78.0|           0|            0|         Yes|      Private|         Urban|            58.57|24.2|        Unknown|     1|\n",
            "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# visualizando dez registros para perceber se a importação foi feita com sucesso\n",
        "df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8dbc0IB54KlY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eef8c0c-59cd-4b22-85ba-b98251948f4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- age: double (nullable = true)\n",
            " |-- hypertension: integer (nullable = true)\n",
            " |-- heart_disease: integer (nullable = true)\n",
            " |-- ever_married: string (nullable = true)\n",
            " |-- work_type: string (nullable = true)\n",
            " |-- Residence_type: string (nullable = true)\n",
            " |-- avg_glucose_level: double (nullable = true)\n",
            " |-- bmi: string (nullable = true)\n",
            " |-- smoking_status: string (nullable = true)\n",
            " |-- stroke: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "CwUpdJDNh6Pd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df901c6c-9fdd-4f76-eb34-b34b43ecfdb6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('id', 'int'),\n",
              " ('gender', 'string'),\n",
              " ('age', 'double'),\n",
              " ('hypertension', 'int'),\n",
              " ('heart_disease', 'int'),\n",
              " ('ever_married', 'string'),\n",
              " ('work_type', 'string'),\n",
              " ('Residence_type', 'string'),\n",
              " ('avg_glucose_level', 'double'),\n",
              " ('bmi', 'string'),\n",
              " ('smoking_status', 'string'),\n",
              " ('stroke', 'int')]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte I: Exploração de Dados.\n",
        "## Analises"
      ],
      "metadata": {
        "id": "7fp0ZS5wIsTa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "W3OcZIaFiA7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b43e0725-6d1e-470e-a9c5-07e851c6a29d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|stroke|count|\n",
            "+------+-----+\n",
            "|     1|  249|\n",
            "|     0| 4861|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Distribuição de casos entre pessoas que tiveram AVC e que não tiveram\n",
        "df.groupBy('stroke').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "qFMRmQp66HV_"
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView('table')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "nGG8HQ_f6Jyn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c48d0bde-123a-4074-8ac1-49efd52e9de3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------------+\n",
            "|    work_type|work_type_count|\n",
            "+-------------+---------------+\n",
            "|      Private|            149|\n",
            "|Self-employed|             65|\n",
            "|     Govt_job|             33|\n",
            "|     children|              2|\n",
            "+-------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Casos pelo tipo de trabalho, trabalhar no setor privado foi o maior\n",
        "spark.sql(\"SELECT work_type, count(work_type) as work_type_count FROM table WHERE stroke == 1 GROUP BY work_type ORDER BY work_type_count DESC\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "oFzqWE6M6KYn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d4010e-264d-4eb3-9aa0-d55dfeedd04e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------+--------------------+\n",
            "|gender|count_gender|             percent|\n",
            "+------+------------+--------------------+\n",
            "|Female|        2994|  58.590998043052835|\n",
            "| Other|           1|0.019569471624266144|\n",
            "|  Male|        2115|    41.3894324853229|\n",
            "+------+------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Quantidade de AVC por genero\n",
        "spark.sql(\"SELECT gender, count(gender) as count_gender, count(gender)*100/sum(count(gender)) over() as percent FROM table GROUP BY gender\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "FLSStOhM6MWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e611e71-1987-446a-e4c7-1d012ddd2a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------+----------------+\n",
            "|gender|count(gender)|      percentage|\n",
            "+------+-------------+----------------+\n",
            "|  Male|          108|5.10638297872340|\n",
            "+------+-------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Porcentagem por genero Masculino\n",
        "spark.sql(\"SELECT gender, count(gender), (COUNT(gender) * 100.0) /(SELECT count(gender) FROM table WHERE gender == 'Male') as percentage FROM table WHERE stroke = '1' and gender = 'Male' GROUP BY gender\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Uyfc2ERe6OzH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f53bf1f-ab25-4a10-e9d8-4b91736096a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------+----------------+\n",
            "|gender|count(gender)|      percentage|\n",
            "+------+-------------+----------------+\n",
            "|Female|          141|4.70941883767535|\n",
            "+------+-------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Porcentagem por genero Feminino\n",
        "spark.sql(\"SELECT gender, count(gender), (COUNT(gender) * 100.0) /(SELECT count(gender) FROM table WHERE gender == 'Female') as percentage FROM table WHERE stroke = '1' and gender = 'Female' GROUP BY gender\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "utq2EVDt6QSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcaa1348-6708-4dff-a09d-704ec4d2fe82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "| age|age_count|\n",
            "+----+---------+\n",
            "|78.0|       21|\n",
            "|79.0|       17|\n",
            "|80.0|       17|\n",
            "|81.0|       14|\n",
            "|57.0|       11|\n",
            "|76.0|       10|\n",
            "|68.0|        9|\n",
            "|63.0|        9|\n",
            "|82.0|        9|\n",
            "|74.0|        9|\n",
            "|59.0|        8|\n",
            "|77.0|        8|\n",
            "|58.0|        7|\n",
            "|71.0|        7|\n",
            "|75.0|        6|\n",
            "|69.0|        6|\n",
            "|70.0|        6|\n",
            "|72.0|        6|\n",
            "|61.0|        6|\n",
            "|54.0|        6|\n",
            "+----+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Distribuição de casos com AVC por idade\n",
        "spark.sql(\"SELECT age, count(age) as age_count FROM table WHERE stroke == 1 GROUP BY age ORDER BY age_count DESC\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "b4G9dQaEifpN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b1aa0cb-4593-4239-9bea-e6ae9a84f12d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade de pessoas com AVC com a idade acima de 60: 177\n"
          ]
        }
      ],
      "source": [
        "idade = 60\n",
        "resultado = df.filter((df['stroke'] == 1) & (df['age'] > idade)).count()\n",
        "print(f'Quantidade de pessoas com AVC com a idade acima de {idade}: {resultado}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhLmYf4bjeP9"
      },
      "source": [
        "## Transformações nos dados"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitui os valores nulos da coluna smoking_status por 'No Info', ou seja sem informação\n",
        "df = df.na.fill('No Info', subset=['smoking_status'])"
      ],
      "metadata": {
        "id": "eHygDnPvSkYo"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "9mni6pmajgEu"
      },
      "outputs": [],
      "source": [
        "# Substitui os valores nulos da coluna bmi, por uma media entre todos os valores\n",
        "mean = df.select(mean(df['bmi'])).collect()\n",
        "mean_bmi = mean[0][0]\n",
        "df = df.na.fill(mean_bmi,['bmi'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "m5JUbRZXkWkU"
      },
      "outputs": [],
      "source": [
        "# Transformas as colunas de texto em valores númericos para utilizadação nos algortimos de classificação\n",
        "gender_indexer = StringIndexer(inputCol='gender', outputCol='genderIndex').setHandleInvalid(\"keep\")\n",
        "gender_encoder = OneHotEncoder(inputCol='genderIndex', outputCol='genderVec')\n",
        "\n",
        "ever_married_indexer = StringIndexer(inputCol='ever_married', outputCol='ever_marriedIndex').setHandleInvalid(\"keep\")\n",
        "ever_married_encoder = OneHotEncoder(inputCol='ever_marriedIndex', outputCol='ever_marriedVec')\n",
        "\n",
        "work_type_indexer = StringIndexer(inputCol='work_type', outputCol='work_typeIndex').setHandleInvalid(\"keep\")\n",
        "work_type_encoder = OneHotEncoder(inputCol='work_typeIndex', outputCol='work_typeVec')\n",
        "\n",
        "Residence_type_indexer = StringIndexer(inputCol='Residence_type', outputCol='Residence_typeIndex').setHandleInvalid(\"keep\")\n",
        "Residence_type_encoder = OneHotEncoder(inputCol='Residence_typeIndex', outputCol='Residence_typeVec')\n",
        "\n",
        "bmi_indexer = StringIndexer(inputCol='bmi', outputCol='bmiIndex').setHandleInvalid(\"keep\")\n",
        "bmi_encoder = OneHotEncoder(inputCol='bmiIndex', outputCol='bmiVec')\n",
        "\n",
        "smoking_status_indexer = StringIndexer(inputCol='smoking_status', outputCol='smoking_statusIndex').setHandleInvalid(\"keep\")\n",
        "smoking_status_encoder = OneHotEncoder(inputCol='smoking_statusIndex', outputCol='smoking_statusVec')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte II: Criação de um Modelo e Análise de Resultados."
      ],
      "metadata": {
        "id": "Q6Jo1z-PT_8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decidimos executar cinco algoritimos de classificação, que encontramos na documentação do Apache Spark\n",
        "\n",
        "- [DecisionTreeClassifier](https://spark.apache.org/docs/latest/api/java/org/apache/spark/ml/classification/DecisionTreeClassifier.html) \n",
        "- [RandomForestClassifier](https://spark.apache.org/docs/latest/ml-classification-regression.html)\n",
        "- [GBTClassifier](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.GBTClassifier.html)\n",
        "- [LinearSVC](https://spark.apache.org/docs/latest/api/java/org/apache/spark/ml/classification/LinearSVC.html)\n",
        "- [NaiveBayes](https://spark.apache.org/docs/latest/mllib-naive-bayes.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "MBWa5yo5UDeA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "yZ6g9e9okAbl"
      },
      "outputs": [],
      "source": [
        "assembler = VectorAssembler(inputCols=['genderVec',\n",
        " 'age',\n",
        " 'hypertension',\n",
        " 'heart_disease',\n",
        " 'ever_marriedVec',\n",
        " 'work_typeVec',\n",
        " 'Residence_typeVec',\n",
        " 'avg_glucose_level',\n",
        " 'bmiVec',\n",
        " 'smoking_statusVec'],outputCol='features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "TfOxhHpekDb1"
      },
      "outputs": [],
      "source": [
        "# Função que recebe o objeto e o nome do algortimo e execução o treinamento e a classificação de sua acurácia\n",
        "def executa_algoritmo(algoritmo, nome_do_algoritmo):\n",
        "  pipeline = Pipeline(stages=[gender_indexer, ever_married_indexer, work_type_indexer, Residence_type_indexer,\n",
        "                            smoking_status_indexer, bmi_indexer, gender_encoder, ever_married_encoder, work_type_encoder,\n",
        "                            Residence_type_encoder, smoking_status_encoder, bmi_encoder, assembler, algoritmo])\n",
        "  \n",
        "  train_data, test_data = df.randomSplit([0.7,0.3])\n",
        "  model = pipeline.fit(train_data)\n",
        "  dtc_predictions = model.transform(test_data)\n",
        "\n",
        "  evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"stroke\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "  accuracy = evaluator.evaluate(dtc_predictions)\n",
        "  print(nome_do_algoritmo)\n",
        "  print('Accuracy of: {0:2.2f}%'.format(accuracy*100))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dtc = DecisionTreeClassifier(labelCol='stroke', featuresCol='features')\n",
        "rfc = RandomForestClassifier(labelCol='stroke', featuresCol='features')\n",
        "gbtc = GBTClassifier(labelCol='stroke', featuresCol='features')\n",
        "lsvc = LinearSVC(labelCol='stroke', featuresCol='features')\n",
        "nb = NaiveBayes(labelCol='stroke', featuresCol='features')\n",
        "\n",
        "# Executa os algoritimos de classificação\n",
        "executa_algoritmo(dtc, \"Decision tree classifier\")\n",
        "print(\"--\" * 20)\n",
        "executa_algoritmo(rfc, \"Random forest classifier\")\n",
        "print(\"--\" * 20)\n",
        "executa_algoritmo(gbtc, \"Gradient-boosted tree classifier\")\n",
        "print(\"--\" * 20)\n",
        "executa_algoritmo(lsvc, \"Linear Support Vector Machine\")\n",
        "print(\"--\" * 20)\n",
        "executa_algoritmo(nb, \"Naive Bayes\")\n",
        "print(\"--\" * 20)"
      ],
      "metadata": {
        "id": "2CPiENf5IK_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d799cc-5ca3-44eb-8a87-cb985375e050"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision tree classifier\n",
            "Accuracy of: 94.67%\n",
            "----------------------------------------\n",
            "Random forest classifier\n",
            "Accuracy of: 95.36%\n",
            "----------------------------------------\n",
            "Gradient-boosted tree classifier\n",
            "Accuracy of: 94.66%\n",
            "----------------------------------------\n",
            "Linear Support Vector Machine\n",
            "Accuracy of: 94.96%\n",
            "----------------------------------------\n",
            "Naive Bayes\n",
            "Accuracy of: 81.58%\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusão\n",
        "\n",
        "A ideia foi comparar algoritmos de classificação de maneira simples e direta, analisando a acurácia de cada um, em que o melhor encontrado foi o \"Random Forest Classifier\", como o proprio nome já diz cria árvores de decisão, de maneira aleatória e chegou em um resultado de 95.47%, um valor proximo ao \"Decision tree classifier\", já os outros algortimos monstraram valores menores"
      ],
      "metadata": {
        "id": "ZxDBObylV4cQ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "projeto-de-disciplina.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}