{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHEv01Di1Wlx"
      },
      "source": [
        "# Big Data e Processamento Distribuído\n",
        "## Projeto de Disciplina"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST6-hWHB1Ywp"
      },
      "source": [
        "Alunos:\n",
        "- André N. Darcie\n",
        "- Cristiane Lemos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF60_wPn13Sx"
      },
      "source": [
        "## Atividade\n",
        "**Valor**: 50 pontos    \n",
        "**Entrega**:  16/01 às 23:59     \n",
        "**Formato**: Jupyter Notebook     \n",
        "Individual ou em duplas.     \n",
        "**Objetivo**: realizar um ciclo de ciência de dados completo no Spark.      \n",
        " \n",
        "Nesse  projeto,  vocês  deverão  realizar  um  ciclo  completo  de  ciência  de  dados  utilizando  o \n",
        "PySpark.  Isso  significa  que  vocês  deverão  explorar  e  preparar  dados,  treinar  um  modelo  de \n",
        "aprendizado de máquina e fazer análise dos resultados obtidos.  \n",
        "Para escolher um dataset, vocês poderão visitar o Kaggle e selecionar algum que estejam com \n",
        "vontade de explorar. Vocês deverão optar por datasets apropriados para as seguintes tarefas: \n",
        "• Classificação \n",
        "• Agrupamento \n",
        "• Recomendação \n",
        "\n",
        "Todo  o  projeto  deve  ser  construído  em  um  único  Notebook.  Nele  deverão  conter,  além  do \n",
        "código,  análises,  explicações  e  motivações  para  a  escolha  do  dataset  e  do  algoritmo  de \n",
        "aprendizado de máquina. IMPORTANTE: Todos os imports utilizados deverão ser colocados no \n",
        "início do Notebook.  \n",
        " \n",
        "### Parte I: Exploração de Dados.\n",
        "\n",
        "Vocês  deverão  utilizar  as  funcionalidades  de  RDD  e/ou  Dataframes  para  analisar  e  limpar  os \n",
        "dados. Como essa tarefa é dependente de cada conjunto de dados, não há um modelo rígido a \n",
        "seguir. Porém, vocês deverão realizar no mínimo 2 análises (estatísticas, análise com gráficos, \n",
        "etc.) e 3 transformações (filtragem, remoção de características, remoção/troca de valores nulos, \n",
        "normalizações, etc). As transformações devem ser pautadas no que for descoberto ao analisar \n",
        "os dados. Por exemplo: normalização dos valores por discrepância de magnitude entre \n",
        "características. \n",
        " \n",
        "### Parte II: Criação de um Modelo e Análise de Resultados. \n",
        "\n",
        "Nessa etapa, vocês deverão rodar um algoritmo da biblioteca MLlib do Spark para aprender um \n",
        "modelo  de  aprendizado  de  máquina  com  os  dados  que  vocês  acabaram  de  organizar.  Vocês \n",
        "deverão motivar a escolha do algoritmo, que deve ser um dos disponíveis dentro da MLlib do \n",
        "Spark. Além disso, vocês deverão dividir os dados utilizando alguma metodologia de validação \n",
        "(cross-validation,  60-40,  80-20,  etc),  e  validar  a  performance  do  seu  modelo,  analisando  os \n",
        "resultados. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4vtbQzV3OHI"
      },
      "source": [
        "## Configuração do ambiente Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APsHuzTE3Cju"
      },
      "source": [
        "Esse notebook é baseado no notebook [como instalar pyspark no google colab](https://colab.research.google.com/github/carlosfab/sigmoidal_ai/blob/master/Big_Data_Como_instalar_o_PySpark_no_Google_Colab.ipynb).       \n",
        "O ambiente de execução é configurado para executar no Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iyig3sF93LM5",
        "outputId": "53e2fecf-23ed-4590-f631-275de98cf7f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:9 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "# instalar as dependências\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# configurar as variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "# tornar o pyspark \"importável\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')\n",
        "\n",
        "import pyspark\n",
        "\n",
        "sc = pyspark.SparkContext(appName='Projeto_BDPD')\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "FlV-ngc6X61K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkUfU-Vf3ZUw"
      },
      "source": [
        "Dataset escolhido: Healthcare Dataset Stroke Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CkWTuzEqbs1W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccd0f276-94e2-4afa-fc9e-a49cf5634567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'my-data-science-notebooks'...\n",
            "remote: Enumerating objects: 1047, done.\u001b[K\n",
            "remote: Counting objects: 100% (635/635), done.\u001b[K\n",
            "remote: Compressing objects: 100% (424/424), done.\u001b[K\n",
            "remote: Total 1047 (delta 360), reused 474 (delta 200), pack-reused 412\u001b[K\n",
            "Receiving objects: 100% (1047/1047), 34.22 MiB | 23.29 MiB/s, done.\n",
            "Resolving deltas: 100% (567/567), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/andredarcie/my-data-science-notebooks.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "sWRLP-nob3q3"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv('/content/my-data-science-notebooks/big-data-processamento-distribuido/datasets/healthcare-dataset-stroke-data.csv', inferSchema=True, header=True, nullValue='NA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Z4ylVPyGdVue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2397e419-08d0-455c-cd9a-9ef295200a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
            "|   id|gender| age|hypertension|heart_disease|ever_married|    work_type|Residence_type|avg_glucose_level| bmi| smoking_status|stroke|\n",
            "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
            "| 9046|  Male|67.0|           0|            1|         Yes|      Private|         Urban|           228.69|36.6|formerly smoked|     1|\n",
            "|51676|Female|61.0|           0|            0|         Yes|Self-employed|         Rural|           202.21| N/A|   never smoked|     1|\n",
            "|31112|  Male|80.0|           0|            1|         Yes|      Private|         Rural|           105.92|32.5|   never smoked|     1|\n",
            "|60182|Female|49.0|           0|            0|         Yes|      Private|         Urban|           171.23|34.4|         smokes|     1|\n",
            "| 1665|Female|79.0|           1|            0|         Yes|Self-employed|         Rural|           174.12|  24|   never smoked|     1|\n",
            "|56669|  Male|81.0|           0|            0|         Yes|      Private|         Urban|           186.21|  29|formerly smoked|     1|\n",
            "|53882|  Male|74.0|           1|            1|         Yes|      Private|         Rural|            70.09|27.4|   never smoked|     1|\n",
            "|10434|Female|69.0|           0|            0|          No|      Private|         Urban|            94.39|22.8|   never smoked|     1|\n",
            "|27419|Female|59.0|           0|            0|         Yes|      Private|         Rural|            76.15| N/A|        Unknown|     1|\n",
            "|60491|Female|78.0|           0|            0|         Yes|      Private|         Urban|            58.57|24.2|        Unknown|     1|\n",
            "+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8dbc0IB54KlY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "256f3e3d-84bf-4188-82f4-810c97fdebb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- age: double (nullable = true)\n",
            " |-- hypertension: integer (nullable = true)\n",
            " |-- heart_disease: integer (nullable = true)\n",
            " |-- ever_married: string (nullable = true)\n",
            " |-- work_type: string (nullable = true)\n",
            " |-- Residence_type: string (nullable = true)\n",
            " |-- avg_glucose_level: double (nullable = true)\n",
            " |-- bmi: string (nullable = true)\n",
            " |-- smoking_status: string (nullable = true)\n",
            " |-- stroke: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CwUpdJDNh6Pd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27f886b3-09e5-4584-e3b8-c596bf7b0707"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('id', 'int'),\n",
              " ('gender', 'string'),\n",
              " ('age', 'double'),\n",
              " ('hypertension', 'int'),\n",
              " ('heart_disease', 'int'),\n",
              " ('ever_married', 'string'),\n",
              " ('work_type', 'string'),\n",
              " ('Residence_type', 'string'),\n",
              " ('avg_glucose_level', 'double'),\n",
              " ('bmi', 'string'),\n",
              " ('smoking_status', 'string'),\n",
              " ('stroke', 'int')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "W3OcZIaFiA7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a418fff-4256-4d6f-9cfb-21ecd7c9bc70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|stroke|count|\n",
            "+------+-----+\n",
            "|     1|  249|\n",
            "|     0| 4861|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupBy('stroke').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qFMRmQp66HV_"
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView('table')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nGG8HQ_f6Jyn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a139491-3162-4cef-e6d7-0b277b7e27b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------------+\n",
            "|    work_type|work_type_count|\n",
            "+-------------+---------------+\n",
            "|      Private|            149|\n",
            "|Self-employed|             65|\n",
            "|     Govt_job|             33|\n",
            "|     children|              2|\n",
            "+-------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT work_type, count(work_type) as work_type_count FROM table WHERE stroke == 1 GROUP BY work_type ORDER BY work_type_count DESC\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "oFzqWE6M6KYn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2c8f38a-0ed0-4271-9f5b-aec66981cf78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------+--------------------+\n",
            "|gender|count_gender|             percent|\n",
            "+------+------------+--------------------+\n",
            "|Female|        2994|  58.590998043052835|\n",
            "| Other|           1|0.019569471624266144|\n",
            "|  Male|        2115|    41.3894324853229|\n",
            "+------+------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT gender, count(gender) as count_gender, count(gender)*100/sum(count(gender)) over() as percent FROM table GROUP BY gender\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FLSStOhM6MWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a8ff506-299f-4b05-8cd0-1a4f7e628b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------+----------------+\n",
            "|gender|count(gender)|      percentage|\n",
            "+------+-------------+----------------+\n",
            "|  Male|          108|5.10638297872340|\n",
            "+------+-------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT gender, count(gender), (COUNT(gender) * 100.0) /(SELECT count(gender) FROM table WHERE gender == 'Male') as percentage FROM table WHERE stroke = '1' and gender = 'Male' GROUP BY gender\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Uyfc2ERe6OzH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aca5c10-cc42-4c41-ef4c-ec9d12fd71a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------+----------------+\n",
            "|gender|count(gender)|      percentage|\n",
            "+------+-------------+----------------+\n",
            "|Female|          141|4.70941883767535|\n",
            "+------+-------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT gender, count(gender), (COUNT(gender) * 100.0) /(SELECT count(gender) FROM table WHERE gender == 'Female') as percentage FROM table WHERE stroke = '1' and gender = 'Female' GROUP BY gender\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "utq2EVDt6QSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "678c9ddd-9b0b-493d-ce4f-30f0d8ebf790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "| age|age_count|\n",
            "+----+---------+\n",
            "|78.0|       21|\n",
            "|79.0|       17|\n",
            "|80.0|       17|\n",
            "|81.0|       14|\n",
            "|57.0|       11|\n",
            "|76.0|       10|\n",
            "|74.0|        9|\n",
            "|63.0|        9|\n",
            "|68.0|        9|\n",
            "|82.0|        9|\n",
            "|59.0|        8|\n",
            "|77.0|        8|\n",
            "|58.0|        7|\n",
            "|71.0|        7|\n",
            "|75.0|        6|\n",
            "|70.0|        6|\n",
            "|69.0|        6|\n",
            "|54.0|        6|\n",
            "|61.0|        6|\n",
            "|72.0|        6|\n",
            "+----+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT age, count(age) as age_count FROM table WHERE stroke == 1 GROUP BY age ORDER BY age_count DESC\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "b4G9dQaEifpN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51400075-91aa-48d3-f68c-ffc162a5b08d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade de pessoas com AVC com a idade acima de 60: 177\n"
          ]
        }
      ],
      "source": [
        "idade = 60\n",
        "resultado = df.filter((df['stroke'] == 1) & (df['age'] > idade)).count()\n",
        "print(f'Quantidade de pessoas com AVC com a idade acima de {idade}: {resultado}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhLmYf4bjeP9"
      },
      "source": [
        "## Limpando os dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9mni6pmajgEu"
      },
      "outputs": [],
      "source": [
        "# fill in missing values\n",
        "df = df.na.fill('No Info', subset=['smoking_status'])\n",
        "\n",
        "# fill in miss values with mean\n",
        "from pyspark.sql.functions import mean\n",
        "mean = df.select(mean(df['bmi'])).collect()\n",
        "mean_bmi = mean[0][0]\n",
        "df = df.na.fill(mean_bmi,['bmi'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "E_ferlv8j_xt"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import (VectorAssembler,OneHotEncoder,StringIndexer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "l7ub-dP6qF-r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82ee7ceb-a845-4857-c75c-6acd2f014ac3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('id', 'int'),\n",
              " ('gender', 'string'),\n",
              " ('age', 'double'),\n",
              " ('hypertension', 'int'),\n",
              " ('heart_disease', 'int'),\n",
              " ('ever_married', 'string'),\n",
              " ('work_type', 'string'),\n",
              " ('Residence_type', 'string'),\n",
              " ('avg_glucose_level', 'double'),\n",
              " ('bmi', 'string'),\n",
              " ('smoking_status', 'string'),\n",
              " ('stroke', 'int')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "m5JUbRZXkWkU"
      },
      "outputs": [],
      "source": [
        "gender_indexer = StringIndexer(inputCol='gender', outputCol='genderIndex').setHandleInvalid(\"keep\")\n",
        "gender_encoder = OneHotEncoder(inputCol='genderIndex', outputCol='genderVec')\n",
        "\n",
        "ever_married_indexer = StringIndexer(inputCol='ever_married', outputCol='ever_marriedIndex').setHandleInvalid(\"keep\")\n",
        "ever_married_encoder = OneHotEncoder(inputCol='ever_marriedIndex', outputCol='ever_marriedVec')\n",
        "\n",
        "work_type_indexer = StringIndexer(inputCol='work_type', outputCol='work_typeIndex').setHandleInvalid(\"keep\")\n",
        "work_type_encoder = OneHotEncoder(inputCol='work_typeIndex', outputCol='work_typeVec')\n",
        "\n",
        "Residence_type_indexer = StringIndexer(inputCol='Residence_type', outputCol='Residence_typeIndex').setHandleInvalid(\"keep\")\n",
        "Residence_type_encoder = OneHotEncoder(inputCol='Residence_typeIndex', outputCol='Residence_typeVec')\n",
        "\n",
        "bmi_indexer = StringIndexer(inputCol='bmi', outputCol='bmiIndex').setHandleInvalid(\"keep\")\n",
        "bmi_encoder = OneHotEncoder(inputCol='bmiIndex', outputCol='bmiVec')\n",
        "\n",
        "smoking_status_indexer = StringIndexer(inputCol='smoking_status', outputCol='smoking_statusIndex').setHandleInvalid(\"keep\")\n",
        "smoking_status_encoder = OneHotEncoder(inputCol='smoking_statusIndex', outputCol='smoking_statusVec')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "yZ6g9e9okAbl"
      },
      "outputs": [],
      "source": [
        "assembler = VectorAssembler(inputCols=['genderVec',\n",
        " 'age',\n",
        " 'hypertension',\n",
        " 'heart_disease',\n",
        " 'ever_marriedVec',\n",
        " 'work_typeVec',\n",
        " 'Residence_typeVec',\n",
        " 'avg_glucose_level',\n",
        " 'bmiVec',\n",
        " 'smoking_statusVec'],outputCol='features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "TfOxhHpekDb1"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "def executa(algoritmo, nome_do_algoritmo):\n",
        "  pipeline = Pipeline(stages=[gender_indexer, ever_married_indexer, work_type_indexer, Residence_type_indexer,\n",
        "                            smoking_status_indexer, bmi_indexer, gender_encoder, ever_married_encoder, work_type_encoder,\n",
        "                            Residence_type_encoder, smoking_status_encoder, bmi_encoder, assembler, algoritmo])\n",
        "  \n",
        "  train_data, test_data = df.randomSplit([0.7,0.3])\n",
        "  model = pipeline.fit(train_data)\n",
        "  dtc_predictions = model.transform(test_data)\n",
        "\n",
        "  evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"stroke\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "  accuracy = evaluator.evaluate(dtc_predictions)\n",
        "  print(nome_do_algoritmo)\n",
        "  print('Accuracy of: {0:2.2f}%'.format(accuracy*100))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "dtc = DecisionTreeClassifier(labelCol='stroke', featuresCol='features')\n",
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "rfc = RandomForestClassifier(labelCol='stroke', featuresCol='features')\n",
        "\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "gbtc = GBTClassifier(labelCol='stroke', featuresCol='features')\n",
        "\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "mpc = MultilayerPerceptronClassifier(labelCol='stroke', featuresCol='features')\n",
        "\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "lsvc = LinearSVC(labelCol='stroke', featuresCol='features')\n",
        "\n",
        "from pyspark.ml.classification import OneVsRest\n",
        "ovr = OneVsRest(labelCol='stroke', featuresCol='features')\n",
        "\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "nb = NaiveBayes(labelCol='stroke', featuresCol='features')\n",
        "\n",
        "#from pyspark.ml.classification import FMClassifier\n",
        "#fmc = FMClassifier(labelCol='stroke', featuresCol='features')\n",
        "\n",
        "executa(dtc, \"Decision tree classifier\")\n",
        "print(\"--\" * 20)\n",
        "executa(rfc, \"Random forest classifier\")\n",
        "print(\"--\" * 20)\n",
        "executa(gbtc, \"Gradient-boosted tree classifier\")\n",
        "print(\"--\" * 20)\n",
        "#executa(mpc, \"MultilayerPerceptronClassifier\")\n",
        "#print(\"--\" * 20)\n",
        "executa(lsvc, \"Linear Support Vector Machine\")\n",
        "print(\"--\" * 20)\n",
        "#executa(ovr, \"One-vs-Rest classifier (a.k.a. One-vs-All)\")\n",
        "#print(\"--\" * 20)\n",
        "executa(nb, \"Naive Bayes\")\n",
        "print(\"--\" * 20)\n",
        "#executa(fmc, \"Factorization machines classifier\")\n",
        "#print(\"--\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CPiENf5IK_W",
        "outputId": "39dcf7a4-220b-4926-ccee-17dd6d60736b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision tree classifier\n",
            "Accuracy of: 95.91%\n",
            "----------------------------------------\n",
            "Random forest classifier\n",
            "Accuracy of: 95.18%\n",
            "----------------------------------------\n",
            "Gradient-boosted tree classifier\n",
            "Accuracy of: 95.05%\n",
            "----------------------------------------\n",
            "Linear Support Vector Machine\n",
            "Accuracy of: 94.74%\n",
            "----------------------------------------\n",
            "Naive Bayes\n",
            "Accuracy of: 79.34%\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "projeto-de-disciplina.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}